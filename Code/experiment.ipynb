{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from constants import TRAIN_TRANSFORM, batch_size, latent_dim, lr, epochs\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from model import VAE\n",
    "from dataset import CelebADataset\n",
    "from scoring import get_fid_score\n",
    "import numpy as np\n",
    "from utils import get_tsne, plot_representation\n",
    "import pandas as pd\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/list_attr_celeba.csv')\n",
    "# df = df.replace(to_replace=-1, value=0)\n",
    "\n",
    "# # list the columns\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 101299\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load and split the CelebA dataset\n",
    "celeba_data_path = './data/img_align_celeba/img_align_celeba/'\n",
    "\n",
    "dataset = CelebADataset(celeba_data_path, transform=TRAIN_TRANSFORM)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "# Create the VAE model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act): ReLU()\n",
       "  (fc_mu): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (fc_logvar): Linear(in_features=4096, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(latent_dim).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./saved_models/vae.pth'))\n",
    "encoder = model.encoder\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "model.eval()\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(val_loader)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m         fid_score\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFID score: \u001b[39m\u001b[39m{\u001b[39;00mfid_score\u001b[39m.\u001b[39mcompute()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m latent_rep \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(latent_rep, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/torchmetrics/metric.py:532\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync_context(\n\u001b[1;32m    528\u001b[0m     dist_sync_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    529\u001b[0m     should_sync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync,\n\u001b[1;32m    530\u001b[0m     should_unsync\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_unsync,\n\u001b[1;32m    531\u001b[0m ):\n\u001b[0;32m--> 532\u001b[0m     value \u001b[39m=\u001b[39m compute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed \u001b[39m=\u001b[39m _squeeze_if_scalar(value)\n\u001b[1;32m    535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_computed\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/torchmetrics/image/fid.py:288\u001b[0m, in \u001b[0;36mFrechetInceptionDistance.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m cov_fake_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_features_cov_sum \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_features_num_samples \u001b[39m*\u001b[39m mean_fake\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mmm(mean_fake)\n\u001b[1;32m    287\u001b[0m cov_fake \u001b[39m=\u001b[39m cov_fake_num \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_features_num_samples \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m \u001b[39mreturn\u001b[39;00m _compute_fid(mean_real\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m), cov_real, mean_fake\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m), cov_fake)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/torchmetrics/image/fid.py:116\u001b[0m, in \u001b[0;36m_compute_fid\u001b[0;34m(mu1, sigma1, mu2, sigma2, eps)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adjusted version of `Fid Score`_\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[39mThe Frechet Inception Distance between two multivariate Gaussians X_x ~ N(mu_1, sigm_1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    Scalar value of the distance between sets.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m diff \u001b[39m=\u001b[39m mu1 \u001b[39m-\u001b[39m mu2\n\u001b[0;32m--> 116\u001b[0m covmean \u001b[39m=\u001b[39m sqrtm(sigma1\u001b[39m.\u001b[39;49mmm(sigma2))\n\u001b[1;32m    117\u001b[0m \u001b[39m# Product might be almost singular\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misfinite(covmean)\u001b[39m.\u001b[39mall():\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/torchmetrics/image/fid.py:72\u001b[0m, in \u001b[0;36mMatrixSquareRoot.forward\u001b[0;34m(ctx, input_data)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx: Any, input_data: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# TODO: update whenever pytorch gets an matrix square root function\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39m# Issue: https://github.com/pytorch/pytorch/issues/9983\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     m \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat_)\n\u001b[0;32m---> 72\u001b[0m     scipy_res, _ \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msqrtm(m, disp\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     73\u001b[0m     sqrtm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(scipy_res\u001b[39m.\u001b[39mreal)\u001b[39m.\u001b[39mto(input_data)\n\u001b[1;32m     74\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(sqrtm)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/scipy/linalg/_matfuncs_sqrtm.py:174\u001b[0m, in \u001b[0;36msqrtm\u001b[0;34m(A, disp, blocksize)\u001b[0m\n\u001b[1;32m    172\u001b[0m keep_it_real \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misrealobj(A)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m keep_it_real:\n\u001b[0;32m--> 174\u001b[0m     T, Z \u001b[39m=\u001b[39m schur(A)\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(T, np\u001b[39m.\u001b[39mtriu(T)):\n\u001b[1;32m    176\u001b[0m         T, Z \u001b[39m=\u001b[39m rsf2csf(T, Z)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_learning/lib/python3.11/site-packages/scipy/linalg/_decomp_schur.py:159\u001b[0m, in \u001b[0;36mschur\u001b[0;34m(a, output, lwork, overwrite_a, sort, check_finite)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39msort\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter must either be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mcallable, or one of (\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlhp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrhp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39miuc\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39mouc\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m result \u001b[39m=\u001b[39m gees(sfunction, a1, lwork\u001b[39m=\u001b[39;49mlwork, overwrite_a\u001b[39m=\u001b[39;49moverwrite_a,\n\u001b[1;32m    160\u001b[0m               sort_t\u001b[39m=\u001b[39;49msort_t)\n\u001b[1;32m    162\u001b[0m info \u001b[39m=\u001b[39m result[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fid_score = FrechetInceptionDistance(n_features=64).to(device)\n",
    "\n",
    "latent_rep = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, label) in enumerate(val_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        x, _, _ = encoder(imgs)\n",
    "        latent_rep.append(x.cpu().numpy())\n",
    "        labels.append(label)\n",
    "        z, _, _ = model(imgs)\n",
    "\n",
    "        fid_score = get_fid_score(fid_score, imgs, z)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f'Batch {i+1}/{len(val_loader)}')\n",
    "\n",
    "print(f'FID score: {fid_score.compute():.4f}')\n",
    "latent_rep = np.concatenate(latent_rep, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "tsne = get_tsne(latent_rep, no_components=2)\n",
    "\n",
    "plot_representation(latent_rep=tsne, labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
